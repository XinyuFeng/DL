{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem 2: Getting familiar with TensorFlow\n",
    "\n",
    "*TensorFlow* is one of the most popular deep learning framework developed by Google. If you are new to TensorFlow, please read and play with the sample in [Getting started with TensorFlow](https://www.tensorflow.org/get_started/get_started) to get started.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 1, you implemented a fully connected network from scratch on your own. Very tedious to do it all by yourself, right? Well, we actually feel the same thing, that's why we are using tools instead of doing everything from scratch, lonely and depressed. For this part of the assignment, we will familiarize you with a widely-used deep learning framework developed by Google, TensorFlow and walk you through convolutional neural networks and show how to train them.\n",
    "* <b>Provided Codes:</b> We provide the Template class for a simple CNN model as BaseModel, predefined skeletons for conv2d() and max_pool(), as well as the dataset preprocessing parts.\n",
    "* <b>TODOs:</b> You are asked to implement the BaseModel following the detailed instrunctions and design your own model in YourModel to achieve a reasonably good performance for classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install and import libraries\n",
    "Install tensorflow and matplotlib.\n",
    "\n",
    "```\n",
    "pip install -U tensorflow matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print \"TensorFlow Version {}\".format(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10923fd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test matplotlib\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "plt.plot(x, np.maximum(0, x), label='relu')\n",
    "plt.plot(x, 1/(1 + np.exp(-x)), label='sigmoid')\n",
    "plt.plot(x, (1 - np.exp(-2 * x))/(1 + np.exp(-2 * x)), label='tanh')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.title(\"Activation functions\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n",
      "2.000000 * 3.000000 = 6.000000\n"
     ]
    }
   ],
   "source": [
    "# Test tensorflow\n",
    "print('TensorFlow version: ' + tf.__version__)\n",
    "a = tf.constant(2.0)\n",
    "b = tf.constant(3.0)\n",
    "c = a * b\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run([a, b, c])\n",
    "print('%f * %f = %f' % (result[0], result[1], result[2]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load datasets\n",
    "Download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) and load the dataset. In this assignment, we will use all 50,000 images for training and 10,000 images for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_training = 49000\n",
    "num_validation = 50000 - num_training\n",
    "num_test = 10000\n",
    "\n",
    "def unpickle(file):\n",
    "    import sys\n",
    "    if sys.version_info.major == 2:\n",
    "        import cPickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "        return dict['data'], dict['labels']\n",
    "    else:\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict[b'data'], dict[b'labels']\n",
    "\n",
    "def load_train_data():\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Load training data from cifar-10 dataset                            #\n",
    "    # Load five files from 'data_batch_1' to 'data_batch_5'                     #\n",
    "    # Reshape images and labels to the shape of [50000, 32, 32, 3]              # \n",
    "    # and [50000], respectively                                                 #\n",
    "    #############################################################################\n",
    "    path = 'data/cifar-10-batches-py/data_batch_'\n",
    "    datas = []\n",
    "    labels = []\n",
    "    for i in range(5):\n",
    "        data, label = unpickle(path + str(i+1))\n",
    "        datas.append(data)\n",
    "        labels.append(label)\n",
    "    datas = np.vstack(datas)\n",
    "    labels = np.vstack(labels)\n",
    "    #labels = np.array(labels)\n",
    "    \n",
    "    datas = datas.reshape(50000, 32, 32, 3)\n",
    "    labels = labels.reshape(50000)\n",
    "    return datas[:num_training, :, :, :], labels[:num_training], datas[num_training:num_training + num_validation, :, :, :], labels[num_training:num_training + num_validation]\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "def load_test_data():\n",
    "    \n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Load testing data from cifar-10 dataset                             #\n",
    "    # Load 'test_batch' file                                                    #\n",
    "    # Reshape images and labels to the shape of [10000, 32, 32, 3]              #\n",
    "    # and [10000], respectively                                                 #\n",
    "    #############################################################################\n",
    "    path = 'data/cifar-10-batches-py/test_batch'\n",
    "    data, label = unpickle(path)\n",
    "    data = np.array(data).reshape(10000, 32, 32, 3)\n",
    "    label = np.array(label).reshape(10000)\n",
    "    return data, label\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "# Load cifar-10 data\n",
    "X_train, Y_train, X_val, Y_val = load_train_data()\n",
    "X_test, Y_test = load_test_data()\n",
    "\n",
    "# Check the shape of the dataset\n",
    "assert X_train.shape == (num_training, 32, 32, 3)\n",
    "assert Y_train.shape == (num_training, )\n",
    "assert X_val.shape == (num_validation, 32, 32, 3)\n",
    "assert Y_val.shape == (num_validation, )\n",
    "assert X_test.shape == (num_test, 32, 32, 3)\n",
    "assert Y_test.shape == (10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2-1\n",
    "\n",
    "Using the code provided, implement a neural network architecture with an optimization routine according to the specification provided below.\n",
    "\n",
    "**Model:**\n",
    "- Input image with the size 32x32x3\n",
    "- 7x7 convolutional layer with 32 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- 5x5 convolutional layer with 64 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- Flatten layer (8x8x64 -> 4096)\n",
    "- Fully-connected layer with 384 output units (4096 -> 384)\n",
    "- ReLU activation layer\n",
    "- Fully-connected layer with 10 output units (384 -> 10)\n",
    "- Output logits (10)\n",
    "\n",
    "**Optimizer:**\n",
    "- Adam optimizer\n",
    "\n",
    "**Learning rate:**\n",
    "- Set start learning rate as 5e-4 and apply exponential decay every 500 steps with a base of 0.96\n",
    "- Use 'tf.train.exponential_decay' and 'tf.train.AdamOptimizer'\n",
    "\n",
    "**Loss:**\n",
    "- Softmax cross entropy loss\n",
    "- Use 'tf.nn.softmax_cross_entropy_with_logits'\n",
    "\n",
    "\n",
    "Your model **should** achieve about 60% accuracy on validation set in 5 epochs using provided evaluation code.\n",
    "\n",
    "You can modify the template code as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define max pooling and conv layers\n",
    "def conv2d(input, kernel_size, stride, num_filter):\n",
    "    stride_shape = [1, stride, stride, 1]\n",
    "    filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "    W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "    b = tf.get_variable('b', [1, 1, 1, num_filter], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.nn.conv2d(input, W, stride_shape, padding='SAME') + b\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "#############################################################################\n",
    "# TODO: You can add any layers (fully-connected, normalization)             #\n",
    "#############################################################################\n",
    "def flatten(input):\n",
    "    return tf.contrib.layers.flatten(input)\n",
    "\n",
    "def fully_con(input, num_outputs):\n",
    "    #flattened_shape = np.array(input.get_shape().as_list()[1:]).prod()\n",
    "    #weights = tf.Variable(tf.truncated_normal([flattened_shape, num_outputs], stddev=1e-4))\n",
    "    #bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    # Fully convolution layer.\n",
    "    #fc = tf.add(tf.matmul(input, weights), bias)  \n",
    "    tmp = tf.contrib.layers.xavier_initializer(uniform=True, seed=9999, dtype=tf.float32)\n",
    "    fc = tf.contrib.layers.fully_connected(input, num_outputs, activation_fn=None)\n",
    "    return fc\n",
    "\n",
    "def loss(logits, y):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sample convolutional nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool2)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fully_con(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fully_con(self.relu3, 10)            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "        \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #self.global_step = tf.placeholder(tf.int32)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        # Adam optimizer 'self.train_op' that minimizes 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(5e-4, self.global_step, 500, 0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss_op, global_step=self.global_step)  \n",
    "        #self.train_op = tf.train.AdamOptimizer().minimize(self.loss_op)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        # Softmax cross entropy loss 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "        self.loss_op = loss(logits, labels)     \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {\n",
    "                    self.X: X_,\n",
    "                    self.Y: Y_,\n",
    "                    self.keep_prob: 1.0\n",
    "                }                \n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            #############################################################################\n",
    "            # TODO: Plot training curves                                                #\n",
    "            #############################################################################\n",
    "            # Graph 1. X: epoch, Y: training loss\n",
    "\n",
    "            # Graph 2. X: epoch, Y: training accuracy\n",
    "            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {\n",
    "                self.X: X_,\n",
    "                self.Y: Y_,\n",
    "                self.keep_prob: 1.0\n",
    "            }\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Sample model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "flat layer: (?, 4096)\n",
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 34.432, accuracy = 0.062\n",
      "iteration (50): loss = 2.092, accuracy = 0.188\n",
      "iteration (100): loss = 1.957, accuracy = 0.250\n",
      "iteration (150): loss = 1.929, accuracy = 0.266\n",
      "iteration (200): loss = 1.820, accuracy = 0.328\n",
      "iteration (250): loss = 1.838, accuracy = 0.336\n",
      "iteration (300): loss = 1.657, accuracy = 0.414\n",
      "iteration (350): loss = 1.613, accuracy = 0.414\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.433\n",
      "train for epoch 1\n",
      "iteration (400): loss = 1.621, accuracy = 0.438\n",
      "iteration (450): loss = 1.600, accuracy = 0.453\n",
      "iteration (500): loss = 1.689, accuracy = 0.383\n",
      "iteration (550): loss = 1.384, accuracy = 0.516\n",
      "iteration (600): loss = 1.677, accuracy = 0.391\n",
      "iteration (650): loss = 1.539, accuracy = 0.461\n",
      "iteration (700): loss = 1.662, accuracy = 0.414\n",
      "iteration (750): loss = 1.414, accuracy = 0.453\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.463\n",
      "train for epoch 2\n",
      "iteration (800): loss = 1.337, accuracy = 0.508\n",
      "iteration (850): loss = 1.580, accuracy = 0.398\n",
      "iteration (900): loss = 1.476, accuracy = 0.453\n",
      "iteration (950): loss = 1.403, accuracy = 0.516\n",
      "iteration (1000): loss = 1.416, accuracy = 0.445\n",
      "iteration (1050): loss = 1.473, accuracy = 0.461\n",
      "iteration (1100): loss = 1.348, accuracy = 0.547\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.483\n",
      "train for epoch 3\n",
      "iteration (1150): loss = 1.376, accuracy = 0.531\n",
      "iteration (1200): loss = 1.412, accuracy = 0.461\n",
      "iteration (1250): loss = 1.581, accuracy = 0.438\n",
      "iteration (1300): loss = 1.501, accuracy = 0.492\n",
      "iteration (1350): loss = 1.294, accuracy = 0.539\n",
      "iteration (1400): loss = 1.406, accuracy = 0.477\n",
      "iteration (1450): loss = 1.360, accuracy = 0.539\n",
      "iteration (1500): loss = 1.184, accuracy = 0.562\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.504\n",
      "train for epoch 4\n",
      "iteration (1550): loss = 1.190, accuracy = 0.586\n",
      "iteration (1600): loss = 1.198, accuracy = 0.578\n",
      "iteration (1650): loss = 1.470, accuracy = 0.461\n",
      "iteration (1700): loss = 1.341, accuracy = 0.500\n",
      "iteration (1750): loss = 1.483, accuracy = 0.484\n",
      "iteration (1800): loss = 1.332, accuracy = 0.508\n",
      "iteration (1850): loss = 1.297, accuracy = 0.523\n",
      "iteration (1900): loss = 1.367, accuracy = 0.508\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.503\n",
      "***** test accuracy: 0.495\n",
      "Model saved in lib/tf_models/problem2/csci-599_sample.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session() as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2-2\n",
    "\n",
    "Implement your own model. \n",
    "\n",
    "You can modify the template code as you want and you can use GPU for fast training.\n",
    "\n",
    "These are the techniques that you can try:\n",
    "- Data preprocessing\n",
    "- Data augmentation\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "- More convolutional layers\n",
    "- More training epochs\n",
    "- Learning rate decay\n",
    "- Any other models and techniqes\n",
    "\n",
    "Your model should achieve >= 70% accuracy on the test set of CIFAR-10.\n",
    "\n",
    "If the accuracy of the model reaches to 80% on the test set, you will get 5 extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    ans = np.zeros(tuple(x.shape))\n",
    "    cnt = x.shape[0]\n",
    "    max_val, min_val = x.max(), x.min()\n",
    "    for i in range(cnt):\n",
    "        ans[i,...] = (x[i, ...] - float(min_val)) / float(max_val - min_val)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class YourModel(object):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.num_epoch = 40\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "        \n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #self.global_step = tf.placeholder(tf.int32)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Your model  ' + '-' * 5)\n",
    "        \n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.conv1 = tf.nn.local_response_normalization(self.conv1)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.conv2 = tf.nn.local_response_normalization(self.conv2)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv3 = conv2d(self.pool2, 5, 1, 128)\n",
    "            self.conv3 = tf.nn.local_response_normalization(self.conv3)\n",
    "            self.relu3 = tf.nn.relu(self.conv3)\n",
    "            self.pool3 = max_pool(self.relu3, 3, 2)            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv3 layer: ' + str(self.pool2.get_shape()))\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool3)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fully_con(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            self.relu3 = tf.nn.dropout(self.relu3, self.keep_prob) \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc4'):\n",
    "            self.fc4 = fully_con(self.relu3, 192)\n",
    "            self.relu4 = tf.nn.relu(self.fc4)\n",
    "            self.dropout = tf.nn.dropout(self.relu4, self.keep_prob) \n",
    "            print('fc4 layer: ' + str(self.dropout.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc5'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc5 = fully_con(self.dropout, 10) \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc5 layer: ' + str(self.fc5.get_shape()))\n",
    "        \n",
    "        return self.fc5\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        #self.global_step = tf.placeholder(tf.int32)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        # Adam optimizer 'self.train_op' that minimizes 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(5e-4, self.global_step, 500, 0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss_op, global_step=self.global_step)  \n",
    "        #self.train_op = tf.train.AdamOptimizer().minimize(self.loss_op)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        # Softmax cross entropy loss 'self.loss_op'\n",
    "        #############################################################################\n",
    "        # TODO: Complete the following functions                                    #\n",
    "        #############################################################################\n",
    "        self.loss_op = loss(logits, labels)     \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {\n",
    "                    self.X: X_,\n",
    "                    self.Y: Y_,\n",
    "                    self.keep_prob: 0.77\n",
    "                }                \n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            #############################################################################\n",
    "            # TODO: Plot training curves                                                #\n",
    "            #############################################################################\n",
    "            # Graph 1. X: epoch, Y: training loss\n",
    "\n",
    "            # Graph 2. X: epoch, Y: training accuracy\n",
    "            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {\n",
    "                self.X: X_,\n",
    "                self.Y: Y_,\n",
    "                self.keep_prob: 1.0\n",
    "            }\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Your model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "conv3 layer: (?, 8, 8, 64)\n",
      "flat layer: (?, 2048)\n",
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 192)\n",
      "fc5 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 2.306, accuracy = 0.094\n",
      "iteration (50): loss = 2.184, accuracy = 0.203\n",
      "iteration (100): loss = 2.009, accuracy = 0.242\n",
      "iteration (150): loss = 1.839, accuracy = 0.305\n",
      "iteration (200): loss = 1.793, accuracy = 0.352\n",
      "iteration (250): loss = 1.718, accuracy = 0.391\n",
      "iteration (300): loss = 1.741, accuracy = 0.375\n",
      "iteration (350): loss = 1.665, accuracy = 0.359\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.451\n",
      "train for epoch 1\n",
      "iteration (400): loss = 1.533, accuracy = 0.398\n",
      "iteration (450): loss = 1.632, accuracy = 0.398\n",
      "iteration (500): loss = 1.610, accuracy = 0.383\n",
      "iteration (550): loss = 1.470, accuracy = 0.422\n",
      "iteration (600): loss = 1.669, accuracy = 0.414\n",
      "iteration (650): loss = 1.520, accuracy = 0.422\n",
      "iteration (700): loss = 1.580, accuracy = 0.438\n",
      "iteration (750): loss = 1.361, accuracy = 0.477\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.497\n",
      "train for epoch 2\n",
      "iteration (800): loss = 1.275, accuracy = 0.484\n",
      "iteration (850): loss = 1.545, accuracy = 0.438\n",
      "iteration (900): loss = 1.388, accuracy = 0.516\n",
      "iteration (950): loss = 1.354, accuracy = 0.547\n",
      "iteration (1000): loss = 1.282, accuracy = 0.570\n",
      "iteration (1050): loss = 1.367, accuracy = 0.500\n",
      "iteration (1100): loss = 1.271, accuracy = 0.516\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.533\n",
      "train for epoch 3\n",
      "iteration (1150): loss = 1.340, accuracy = 0.516\n",
      "iteration (1200): loss = 1.319, accuracy = 0.539\n",
      "iteration (1250): loss = 1.462, accuracy = 0.508\n",
      "iteration (1300): loss = 1.396, accuracy = 0.539\n",
      "iteration (1350): loss = 1.142, accuracy = 0.547\n",
      "iteration (1400): loss = 1.374, accuracy = 0.500\n",
      "iteration (1450): loss = 1.271, accuracy = 0.492\n",
      "iteration (1500): loss = 1.115, accuracy = 0.562\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.535\n",
      "train for epoch 4\n",
      "iteration (1550): loss = 1.100, accuracy = 0.617\n",
      "iteration (1600): loss = 1.186, accuracy = 0.578\n",
      "iteration (1650): loss = 1.201, accuracy = 0.570\n",
      "iteration (1700): loss = 1.225, accuracy = 0.547\n",
      "iteration (1750): loss = 1.286, accuracy = 0.523\n",
      "iteration (1800): loss = 1.090, accuracy = 0.633\n",
      "iteration (1850): loss = 1.249, accuracy = 0.586\n",
      "iteration (1900): loss = 1.195, accuracy = 0.531\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.577\n",
      "train for epoch 5\n",
      "iteration (1950): loss = 1.250, accuracy = 0.594\n",
      "iteration (2000): loss = 0.976, accuracy = 0.617\n",
      "iteration (2050): loss = 1.149, accuracy = 0.602\n",
      "iteration (2100): loss = 1.192, accuracy = 0.586\n",
      "iteration (2150): loss = 1.235, accuracy = 0.602\n",
      "iteration (2200): loss = 1.294, accuracy = 0.492\n",
      "iteration (2250): loss = 1.072, accuracy = 0.570\n",
      "validation for epoch 5\n",
      "-  epoch 5: validation accuracy = 0.588\n",
      "train for epoch 6\n",
      "iteration (2300): loss = 1.100, accuracy = 0.609\n",
      "iteration (2350): loss = 1.140, accuracy = 0.625\n",
      "iteration (2400): loss = 1.084, accuracy = 0.648\n",
      "iteration (2450): loss = 1.202, accuracy = 0.555\n",
      "iteration (2500): loss = 1.122, accuracy = 0.609\n",
      "iteration (2550): loss = 1.259, accuracy = 0.531\n",
      "iteration (2600): loss = 1.061, accuracy = 0.633\n",
      "iteration (2650): loss = 1.058, accuracy = 0.555\n",
      "validation for epoch 6\n",
      "-  epoch 6: validation accuracy = 0.588\n",
      "train for epoch 7\n",
      "iteration (2700): loss = 1.088, accuracy = 0.648\n",
      "iteration (2750): loss = 1.056, accuracy = 0.594\n",
      "iteration (2800): loss = 1.096, accuracy = 0.602\n",
      "iteration (2850): loss = 0.885, accuracy = 0.711\n",
      "iteration (2900): loss = 1.085, accuracy = 0.625\n",
      "iteration (2950): loss = 0.955, accuracy = 0.656\n",
      "iteration (3000): loss = 0.782, accuracy = 0.750\n",
      "iteration (3050): loss = 0.916, accuracy = 0.680\n",
      "validation for epoch 7\n",
      "-  epoch 7: validation accuracy = 0.595\n",
      "train for epoch 8\n",
      "iteration (3100): loss = 0.934, accuracy = 0.688\n",
      "iteration (3150): loss = 1.175, accuracy = 0.609\n",
      "iteration (3200): loss = 1.133, accuracy = 0.625\n",
      "iteration (3250): loss = 1.121, accuracy = 0.602\n",
      "iteration (3300): loss = 0.899, accuracy = 0.703\n",
      "iteration (3350): loss = 1.073, accuracy = 0.648\n",
      "iteration (3400): loss = 1.080, accuracy = 0.586\n",
      "validation for epoch 8\n",
      "-  epoch 8: validation accuracy = 0.603\n",
      "train for epoch 9\n",
      "iteration (3450): loss = 0.994, accuracy = 0.695\n",
      "iteration (3500): loss = 0.889, accuracy = 0.734\n",
      "iteration (3550): loss = 0.916, accuracy = 0.664\n",
      "iteration (3600): loss = 0.925, accuracy = 0.711\n",
      "iteration (3650): loss = 1.005, accuracy = 0.625\n",
      "iteration (3700): loss = 1.178, accuracy = 0.570\n",
      "iteration (3750): loss = 0.859, accuracy = 0.703\n",
      "iteration (3800): loss = 0.886, accuracy = 0.727\n",
      "validation for epoch 9\n",
      "-  epoch 9: validation accuracy = 0.606\n",
      "train for epoch 10\n",
      "iteration (3850): loss = 0.905, accuracy = 0.680\n",
      "iteration (3900): loss = 0.885, accuracy = 0.711\n",
      "iteration (3950): loss = 0.949, accuracy = 0.703\n",
      "iteration (4000): loss = 0.808, accuracy = 0.758\n",
      "iteration (4050): loss = 0.703, accuracy = 0.727\n",
      "iteration (4100): loss = 0.758, accuracy = 0.727\n",
      "iteration (4150): loss = 0.644, accuracy = 0.797\n",
      "iteration (4200): loss = 0.847, accuracy = 0.773\n",
      "validation for epoch 10\n",
      "-  epoch 10: validation accuracy = 0.621\n",
      "train for epoch 11\n",
      "iteration (4250): loss = 0.936, accuracy = 0.641\n",
      "iteration (4300): loss = 0.873, accuracy = 0.695\n",
      "iteration (4350): loss = 0.856, accuracy = 0.727\n",
      "iteration (4400): loss = 0.797, accuracy = 0.734\n",
      "iteration (4450): loss = 0.854, accuracy = 0.656\n",
      "iteration (4500): loss = 0.781, accuracy = 0.734\n",
      "iteration (4550): loss = 1.010, accuracy = 0.672\n",
      "validation for epoch 11\n",
      "-  epoch 11: validation accuracy = 0.629\n",
      "train for epoch 12\n",
      "iteration (4600): loss = 0.707, accuracy = 0.773\n",
      "iteration (4650): loss = 0.857, accuracy = 0.695\n",
      "iteration (4700): loss = 0.807, accuracy = 0.680\n",
      "iteration (4750): loss = 0.872, accuracy = 0.703\n",
      "iteration (4800): loss = 0.769, accuracy = 0.734\n",
      "iteration (4850): loss = 0.674, accuracy = 0.773\n",
      "iteration (4900): loss = 0.851, accuracy = 0.680\n",
      "iteration (4950): loss = 0.911, accuracy = 0.656\n",
      "validation for epoch 12\n",
      "-  epoch 12: validation accuracy = 0.628\n",
      "train for epoch 13\n",
      "iteration (5000): loss = 0.741, accuracy = 0.727\n",
      "iteration (5050): loss = 0.732, accuracy = 0.742\n",
      "iteration (5100): loss = 0.644, accuracy = 0.789\n",
      "iteration (5150): loss = 0.647, accuracy = 0.758\n",
      "iteration (5200): loss = 0.759, accuracy = 0.734\n",
      "iteration (5250): loss = 0.772, accuracy = 0.727\n",
      "iteration (5300): loss = 0.786, accuracy = 0.758\n",
      "validation for epoch 13\n",
      "-  epoch 13: validation accuracy = 0.626\n",
      "train for epoch 14\n",
      "iteration (5350): loss = 0.729, accuracy = 0.758\n",
      "iteration (5400): loss = 0.572, accuracy = 0.797\n",
      "iteration (5450): loss = 0.637, accuracy = 0.789\n",
      "iteration (5500): loss = 0.787, accuracy = 0.711\n",
      "iteration (5550): loss = 0.737, accuracy = 0.680\n",
      "iteration (5600): loss = 0.780, accuracy = 0.750\n",
      "iteration (5650): loss = 0.565, accuracy = 0.812\n",
      "iteration (5700): loss = 0.801, accuracy = 0.672\n",
      "validation for epoch 14\n",
      "-  epoch 14: validation accuracy = 0.613\n",
      "train for epoch 15\n",
      "iteration (5750): loss = 0.533, accuracy = 0.820\n",
      "iteration (5800): loss = 0.773, accuracy = 0.711\n",
      "iteration (5850): loss = 0.692, accuracy = 0.734\n",
      "iteration (5900): loss = 0.624, accuracy = 0.781\n",
      "iteration (5950): loss = 0.508, accuracy = 0.805\n",
      "iteration (6000): loss = 0.722, accuracy = 0.719\n",
      "iteration (6050): loss = 0.661, accuracy = 0.742\n",
      "iteration (6100): loss = 0.705, accuracy = 0.766\n",
      "validation for epoch 15\n",
      "-  epoch 15: validation accuracy = 0.596\n",
      "train for epoch 16\n",
      "iteration (6150): loss = 0.633, accuracy = 0.766\n",
      "iteration (6200): loss = 0.705, accuracy = 0.773\n",
      "iteration (6250): loss = 0.733, accuracy = 0.727\n",
      "iteration (6300): loss = 0.574, accuracy = 0.797\n",
      "iteration (6350): loss = 0.659, accuracy = 0.734\n",
      "iteration (6400): loss = 0.751, accuracy = 0.688\n",
      "iteration (6450): loss = 0.554, accuracy = 0.820\n",
      "validation for epoch 16\n",
      "-  epoch 16: validation accuracy = 0.608\n",
      "train for epoch 17\n",
      "iteration (6500): loss = 0.671, accuracy = 0.758\n",
      "iteration (6550): loss = 0.510, accuracy = 0.805\n",
      "iteration (6600): loss = 0.821, accuracy = 0.727\n",
      "iteration (6650): loss = 0.620, accuracy = 0.773\n",
      "iteration (6700): loss = 0.499, accuracy = 0.820\n",
      "iteration (6750): loss = 0.735, accuracy = 0.766\n",
      "iteration (6800): loss = 0.578, accuracy = 0.773\n",
      "iteration (6850): loss = 0.615, accuracy = 0.766\n",
      "validation for epoch 17\n",
      "-  epoch 17: validation accuracy = 0.594\n",
      "train for epoch 18\n",
      "iteration (6900): loss = 0.626, accuracy = 0.781\n",
      "iteration (6950): loss = 0.525, accuracy = 0.820\n",
      "iteration (7000): loss = 0.740, accuracy = 0.727\n",
      "iteration (7050): loss = 0.677, accuracy = 0.766\n",
      "iteration (7100): loss = 0.555, accuracy = 0.805\n",
      "iteration (7150): loss = 0.496, accuracy = 0.844\n",
      "iteration (7200): loss = 0.709, accuracy = 0.742\n",
      "iteration (7250): loss = 0.562, accuracy = 0.820\n",
      "validation for epoch 18\n",
      "-  epoch 18: validation accuracy = 0.604\n",
      "train for epoch 19\n",
      "iteration (7300): loss = 0.444, accuracy = 0.844\n",
      "iteration (7350): loss = 0.543, accuracy = 0.828\n",
      "iteration (7400): loss = 0.580, accuracy = 0.789\n",
      "iteration (7450): loss = 0.463, accuracy = 0.836\n",
      "iteration (7500): loss = 0.407, accuracy = 0.844\n",
      "iteration (7550): loss = 0.439, accuracy = 0.844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-12d7e3e02235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYourModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'***** test accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-cd006fbfc686>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fengxinyu/Downloads/assignment1-2/assignment/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fengxinyu/Downloads/assignment1-2/assignment/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fengxinyu/Downloads/assignment1-2/assignment/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fengxinyu/Downloads/assignment1-2/assignment/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fengxinyu/Downloads/assignment1-2/assignment/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Preprocessing                                                       #\n",
    "#############################################################################\n",
    "X_train_ = normalize(X_train)\n",
    "X_val_ = normalize(X_val)\n",
    "X_test_ = normalize(X_test)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "model = YourModel()\n",
    "model.train(sess, X_train_, Y_train, X_val_, Y_val)\n",
    "accuracy = model.evaluate(sess, X_test_, Y_test)\n",
    "print('***** test accuracy: %.3f' % accuracy)\n",
    "\n",
    "# Save your model\n",
    "saver = tf.train.Saver()\n",
    "model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")\n",
    "print(\"Model saved in %s\" % model_path)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load your model\n",
    "model = YourModel()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
